#app.py
import streamlit as st
import pandas as pd
import requests
import json
import time
from io import BytesIO

# Page configuration
st.set_page_config(
    page_title="Databricks ML Pipeline", 
    layout="wide",
    page_icon="ğŸš€"
)

def initialize_session_state():
    """Initialize session state variables"""
    if 'job_status' not in st.session_state:
        st.session_state.job_status = 'not_started'
    if 'run_id' not in st.session_state:
        st.session_state.run_id = None
    if 'table_name' not in st.session_state:
        st.session_state.table_name = "diabetes_data_corrected"

def get_databricks_config():
    """Get Databricks configuration from secrets"""
    try:
        config = {
            'host': st.secrets["DATABRICKS"]["HOST"].rstrip('/'),
            'token': st.secrets["DATABRICKS"]["TOKEN"],
            'job_id': st.secrets["DATABRICKS"]["JOB_ID"]
        }
        return config
    except Exception as e:
        st.error(f"âŒ Error loading Databricks configuration: {e}")
        return None

def trigger_databricks_job(config, table_name, model_type, enable_tuning, test_size):
    """Trigger Databricks job via API"""
    try:
        url = f"{config['host']}/api/2.0/jobs/run-now"
        
        headers = {
            "Authorization": f"Bearer {config['token']}",
            "Content-Type": "application/json"
        }
        
        # Job parameters
        data = {
            "job_id": int(config['job_id']),
            "notebook_params": {
                "table_name": table_name,
                "model_type": model_type,
                "enable_tuning": str(enable_tuning).lower(),
                "test_size": str(test_size),
                "output_path": "dbfs:/FileStore/results"
            }
        }
        
        response = requests.post(url, headers=headers, json=data)
        
        if response.status_code == 200:
            run_id = response.json()["run_id"]
            return run_id
        else:
            st.error(f"âŒ Job trigger failed: {response.text}")
            return None
            
    except Exception as e:
        st.error(f"âŒ Error triggering job: {e}")
        return None

def get_job_status(config, run_id):
    """Get job status"""
    try:
        url = f"{config['host']}/api/2.0/jobs/runs/get?run_id={run_id}"
        
        headers = {
            "Authorization": f"Bearer {config['token']}"
        }
        
        response = requests.get(url, headers=headers)
        
        if response.status_code == 200:
            result = response.json()
            state = result["state"]
            return {
                "life_cycle_state": state["life_cycle_state"],
                "result_state": state.get("result_state", "UNKNOWN"),
                "state_message": state.get("state_message", "")
            }
        else:
            return {
                "life_cycle_state": "UNKNOWN",
                "result_state": "FAILED", 
                "state_message": f"API Error: {response.text}"
            }
            
    except Exception as e:
        return {
            "life_cycle_state": "ERROR",
            "result_state": "FAILED",
            "state_message": str(e)
        }

def run_pipeline(table_name, model_name, enable_tuning, test_size):
    """Trigger the Databricks pipeline"""
    try:
        config = get_databricks_config()
        if not config:
            st.error("âŒ Cannot start pipeline - Databricks configuration missing")
            return
        
        # Show progress
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # Map model names to internal codes
        model_mapping = {
            "Logistic Regression": "logistic",
            "Random Forest": "random_forest", 
            "Neural Network": "neural_net"
        }
        
        model_code = model_mapping[model_name]
        
        # Step 1: Trigger job
        status_text.info("ğŸš€ Starting ML pipeline on Databricks...")
        run_id = trigger_databricks_job(config, table_name, model_code, enable_tuning, test_size)
        progress_bar.progress(30)
        
        if not run_id:
            st.error("âŒ Failed to start Databricks job.")
            return
        
        # Store in session state
        st.session_state.run_id = run_id
        st.session_state.job_status = 'running'
        
        # Step 2: Poll for completion
        status_text.info("ğŸ”„ Pipeline running... This may take a few minutes.")
        
        max_attempts = 120  # 10 minutes max
        for attempt in range(max_attempts):
            status_info = get_job_status(config, run_id)
            life_cycle_state = status_info["life_cycle_state"]
            
            # Update progress based on state
            if life_cycle_state == "PENDING":
                progress = 0.3 + (attempt / max_attempts) * 0.2
                status_text.info("â³ Job queued...")
            elif life_cycle_state == "RUNNING":
                progress = 0.5 + (attempt / max_attempts) * 0.4
                status_text.info("ğŸ¤– Processing data and training model...")
            else:
                progress = 0.9
            
            progress_bar.progress(min(progress, 0.9))
            
            if life_cycle_state in ["TERMINATED", "SKIPPED", "INTERNAL_ERROR"]:
                break
                
            time.sleep(5)
        
        progress_bar.progress(1.0)
        
        if life_cycle_state == "TERMINATED" and status_info["result_state"] == "SUCCESS":
            status_text.success("âœ… Pipeline completed successfully!")
            st.session_state.job_status = 'completed'
            show_results_section(config, run_id)
        else:
            status_text.error(f"âŒ Pipeline ended with status: {life_cycle_state}")
            st.error(f"Error: {status_info['state_message']}")
            st.session_state.job_status = 'failed'
        
    except Exception as e:
        st.error(f"âŒ Error in pipeline: {e}")
        st.session_state.job_status = 'failed'

def show_results_section(config, run_id):
    """Display results from the completed job"""
    try:
        st.markdown("---")
        st.header("ğŸ“Š Pipeline Results")
        
        # Get job output
        url = f"{config['host']}/api/2.0/jobs/runs/get-output?run_id={run_id}"
        headers = {"Authorization": f"Bearer {config['token']}"}
        
        response = requests.get(url, headers=headers)
        
        if response.status_code == 200:
            output = response.json()
            if "notebook_output" in output and output["notebook_output"]:
                st.subheader("Execution Logs")
                logs = output["notebook_output"]["result"] if isinstance(output["notebook_output"], dict) else output["notebook_output"]
                st.text_area("Logs", logs, height=200)
            
            # Try to get results from DBFS
            try:
                results_url = f"{config['host']}/api/2.0/dbfs/read"
                headers = {"Authorization": f"Bearer {config['token']}"}
                results_data = {"path": "/FileStore/results/results.json"}
                
                response = requests.get(results_url, headers=headers, json=results_data)
                if response.status_code == 200:
                    import base64
                    results_content = base64.b64decode(response.json()["data"]).decode('utf-8')
                    results = json.loads(results_content)
                    
                    if results.get("status") == "success":
                        st.subheader("ğŸ“ˆ Model Performance")
                        metrics = results.get("metrics", {})
                        
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("Accuracy", f"{metrics.get('accuracy', 0):.4f}")
                        with col2:
                            st.metric("Precision", f"{metrics.get('precision', 0):.4f}")
                        with col3:
                            st.metric("Recall", f"{metrics.get('recall', 0):.4f}")
                        with col4:
                            st.metric("F1 Score", f"{metrics.get('f1_score', 0):.4f}")
                        
                        if "roc_auc" in metrics:
                            st.metric("ROC AUC", f"{metrics.get('roc_auc', 0):.4f}")
                        
                        # Show dataset info
                        st.subheader("ğŸ“‹ Dataset Info")
                        col1, col2 = st.columns(2)
                        with col1:
                            st.info(f"**Table:** {results.get('data_source', 'N/A')}")
                            st.info(f"**Model:** {results.get('model_type', 'N/A')}")
                        with col2:
                            if 'eda' in results:
                                st.info(f"**Rows:** {results['eda'].get('shape', [0, 0])[0]:,}")
                                st.info(f"**Features:** {results['eda'].get('shape', [0, 0])[1] - 1}")
                        
                        st.success("ğŸ‰ Model training completed successfully!")
            except Exception as e:
                st.info("ğŸ“Š Check Databricks MLflow for detailed metrics")
                
        else:
            st.info("ğŸ“‹ Check Databricks workspace for complete results")
            
    except Exception as e:
        st.error(f"Error fetching results: {e}")

def main():
    initialize_session_state()
    
    st.title("ğŸš€ Databricks ML Pipeline")
    st.markdown("Train ML models on Diabetes Prediction Dataset!")
    
    # Check configurations
    databricks_config = get_databricks_config()
        
    if not databricks_config:
        st.error("âŒ Databricks configuration missing.")
        return
    
    # Sidebar for configuration
    with st.sidebar:
        st.header("âš™ï¸ Configuration")
        
        # Model selection
        model_options = [
            "Logistic Regression",
            "Random Forest", 
            "Neural Network"
        ]
        
        selected_model = st.selectbox("Select Model", options=model_options, index=1)
        enable_tuning = st.checkbox("Enable Hyperparameter Tuning", value=False)
        test_size = st.slider("Test Set Size (%)", 10, 40, 20)
        
        st.markdown("---")
        st.header("ğŸ“Š Current Setup")
        st.success(f"âœ… Using Table: **{st.session_state.table_name}**")
        st.write(f"**Model:** {selected_model}")
        st.write(f"**Test Size:** {test_size}%")
        st.write(f"**Tuning:** {'Yes' if enable_tuning else 'No'}")
        
        st.markdown("---")
        st.header("ğŸ“ˆ Dataset Info")
        st.info("""
        **Diabetes Prediction Dataset:**
        - **Target:** Diabetes_binary (0.0 = No, 1.0 = Yes)
        - **Samples:** 253,681
        - **Features:** 21 health indicators
        - **Classes:** 0.0 (86%), 1.0 (14%)
        """)
        
        st.markdown("---")
        st.header("â„¹ï¸ How It Works")
        st.info("""
        **Process:**
        1. Configure ML settings
        2. Start pipeline with diabetes dataset
        3. Databricks trains model on health data
        4. View prediction results
        
        **Features:**
        - HighBP, HighChol, BMI, Smoker
        - Stroke, HeartDisease, PhysActivity
        - Fruits, Veggies, AlcoholConsump
        """)
    
    # Main area
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.header("ğŸ“‹ Dataset Preview")
        st.info(f"""
        **Using Table:** `{st.session_state.table_name}`
        
        **Target Variable:** `Diabetes_binary`
        - **0.0**: No Diabetes (218,334 samples)
        - **1.0**: Has Diabetes (35,346 samples)
        
        **Health Indicators:**
        - HighBP, HighChol, CholCheck, BMI
        - Smoker, Stroke, HeartDiseaseorAttack
        - PhysActivity, Fruits, Veggies, HvyAlcoholConsump
        """)
        
        # Show table preview
        if st.button("ğŸ” Preview Data", use_container_width=True):
            try:
                config = get_databricks_config()
                if config:
                    # Execute SQL to get table preview
                    sql_url = f"{config['host']}/api/2.0/sql/statements"
                    headers = {"Authorization": f"Bearer {config['token']}"}
                    
                    sql_data = {
                        "statement": f"""
                        SELECT Diabetes_binary, HighBP, HighChol, BMI, Smoker, 
                               Stroke, HeartDiseaseorAttack, PhysActivity
                        FROM hive_metastore.default.{st.session_state.table_name} 
                        LIMIT 8
                        """,
                        "warehouse_id": "auto"
                    }
                    
                    response = requests.post(sql_url, headers=headers, json=sql_data)
                    if response.status_code == 200:
                        result = response.json()
                        if 'result' in result and 'data_array' in result['result']:
                            data = result['result']['data_array']
                            columns = [col['name'] for col in result['result']['manifest']['schema']['columns']]
                            preview_df = pd.DataFrame(data, columns=columns)
                            st.dataframe(preview_df)
            except Exception as e:
                st.error(f"Could not fetch preview: {e}")
    
    with col2:
        st.header("ğŸš€ Run ML Pipeline")
        
        st.write("**Pipeline Configuration:**")
        st.write(f"- **Dataset:** Diabetes Prediction")
        st.write(f"- **Samples:** 253,681 patients")
        st.write(f"- **Target:** Diabetes Detection")
        st.write(f"- **Model:** {selected_model}")
        st.write(f"- **Test Size:** {test_size}%")
        st.write(f"- **Tuning:** {'Yes' if enable_tuning else 'No'}")
        
        if st.button("ğŸ¯ Start Diabetes Prediction Pipeline", type="primary", use_container_width=True):
            run_pipeline(st.session_state.table_name, selected_model, enable_tuning, test_size)
        
        # Show status
        if st.session_state.job_status == 'running':
            st.info("ğŸ”„ Pipeline is running...")
            st.info("ğŸ’¡ Training model on diabetes dataset...")
        elif st.session_state.job_status == 'completed':
            st.success("âœ… Pipeline completed!")
        elif st.session_state.job_status == 'failed':
            st.error("âŒ Pipeline failed.")

if __name__ == "__main__":
    main()

#dbx_utils
# dbx_utils.py
import os
import requests
import json
from typing import Dict

DATABRICKS_HOST = os.environ.get("DATABRICKS_HOST")  # e.g. "https://<your-workspace>.cloud.databricks.com"
DATABRICKS_TOKEN = os.environ.get("DATABRICKS_TOKEN")

if not DATABRICKS_HOST or not DATABRICKS_TOKEN:
    # don't raise here; streamlit will show instructions to set them
    pass

HEADERS = {
    "Authorization": f"Bearer {DATABRICKS_TOKEN}"
}

def upload_file_to_dbfs(local_path: str, dbfs_path: str):
    """
    Upload local file to DBFS using the 2-step create+put API (small files supported).
    Uses simple single PUT approach. For large files you can chunk.
    """
    # read bytes
    with open(local_path, "rb") as f:
        data = f.read()

    url = f"{DATABRICKS_HOST}/api/2.0/dbfs/put"
    resp = requests.post(url, headers=HEADERS, json={
        "path": dbfs_path,
        "contents": data.hex(),  # DBFS put expects base64, but hex won't work for every workspace; fallback to base64
    })
    # Some workspaces expect base64; if server returns 400 try base64:
    if resp.status_code != 200:
        import base64
        resp = requests.post(url, headers=HEADERS, json={
            "path": dbfs_path,
            "contents": base64.b64encode(data).decode("utf-8"),
        })
    resp.raise_for_status()
    return resp.json()

def run_job_now(job_name: str, notebook_params: Dict = None) -> Dict:
    """
    Starts a run of existing job by name (keeps job name the same).
    Returns run_id and response.
    """
    # First find job id by name
    search_url = f"{DATABRICKS_HOST}/api/2.1/jobs/list"
    r = requests.get(search_url, headers=HEADERS)
    r.raise_for_status()
    jobs = r.json().get("jobs", [])
    job_id = None
    for j in jobs:
        if j.get("settings", {}).get("name") == job_name:
            job_id = j["job_id"]
            break
    if job_id is None:
        raise ValueError(f"Job with name '{job_name}' not found in Databricks workspace.")

    run_url = f"{DATABRICKS_HOST}/api/2.1/jobs/run-now"
    payload = {"job_id": job_id}
    if notebook_params:
        payload["notebook_params"] = notebook_params
    r2 = requests.post(run_url, headers=HEADERS, json=payload)
    r2.raise_for_status()
    return r2.json()

#ml_pipeline_main.py
#!/usr/bin/env python3
# Databricks notebook source
# ML Pipeline - Diabetes Prediction

# COMMAND ----------

# MAGIC %md
# MAGIC # ğŸš€ Diabetes Prediction ML Pipeline
# MAGIC 
# MAGIC **Predicts diabetes based on health indicators**

# COMMAND ----------

# Import libraries
import pandas as pd
import numpy as np
from pyspark.sql import SparkSession
import mlflow
import mlflow.sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
import json
import os
from datetime import datetime

# COMMAND ----------

# MAGIC %md
# MAGIC ## âš™ï¸ Configuration

# COMMAND ----------

# Create widgets for parameters
dbutils.widgets.text("table_name", "diabetes_data_corrected", "Table Name")
dbutils.widgets.dropdown("model_type", "random_forest", ["logistic", "random_forest", "neural_net"], "Model Type")
dbutils.widgets.dropdown("enable_tuning", "false", ["true", "false"], "Enable Tuning")
dbutils.widgets.text("test_size", "20", "Test Size (%)")
dbutils.widgets.text("output_path", "dbfs:/FileStore/results", "Output Results Path")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸš€ ML Pipeline Functions

# COMMAND ----------

def load_data_from_table(table_name):
    """Load data from Delta Table"""
    print("ğŸ“ Loading data from Delta Table...")
    print(f"Table Name: {table_name}")
    
    try:
        full_table_name = f"hive_metastore.default.{table_name}"
        df_spark = spark.sql(f"SELECT * FROM {full_table_name}")
        df = df_spark.toPandas()
        
        print(f"âœ… Data loaded successfully!")
        print(f"ğŸ“Š Shape: {df.shape}")
        print(f"ğŸ¯ Columns: {df.columns.tolist()}")
        
        return df
        
    except Exception as e:
        print(f"âŒ Error loading data from table: {str(e)}")
        raise e

def perform_eda(df):
    """Perform Exploratory Data Analysis"""
    print("ğŸ“Š Performing EDA...")
    
    # Calculate basic statistics
    eda_results = {
        "shape": df.shape,
        "columns": df.columns.tolist(),
        "data_types": str(df.dtypes.to_dict()),
        "missing_values": df.isnull().sum().to_dict(),
        "target_distribution": None,
        "feature_summary": {}
    }
    
    # Target analysis
    target_col = 'Diabetes_binary'
    if target_col in df.columns:
        target_dist = df[target_col].value_counts().to_dict()
        eda_results["target_distribution"] = target_dist
        eda_results["target_name"] = target_col
        
        print(f"ğŸ¯ Target column: {target_col}")
        print(f"ğŸ“ˆ Target distribution: {target_dist}")
        print(f"ğŸ”¢ Class ratio: {target_dist.get(0.0, 0)} (No Diabetes) vs {target_dist.get(1.0, 0)} (Diabetes)")
    
    # Feature analysis
    feature_cols = [col for col in df.columns if col != target_col]
    for col in feature_cols[:10]:  # First 10 features
        if df[col].dtype in ['int64', 'float64']:
            eda_results["feature_summary"][col] = {
                "mean": float(df[col].mean()),
                "std": float(df[col].std()),
                "min": float(df[col].min()),
                "max": float(df[col].max())
            }
    
    print("âœ… EDA completed")
    return eda_results

def preprocess_data(df, test_size=0.2):
    """Preprocess the diabetes data for ML"""
    print("ğŸ”§ Preprocessing data...")
    
    target_col = 'Diabetes_binary'
    feature_cols = [col for col in df.columns if col != target_col]
    
    X = df[feature_cols]
    y = df[target_col]
    
    print(f"Original - Features: {X.shape}, Target: {y.shape}")
    print(f"ğŸ¯ Target distribution: {y.value_counts().to_dict()}")
    
    # Handle missing values
    missing_before = X.isnull().sum().sum()
    if missing_before > 0:
        print(f"ğŸ”„ Handling {missing_before} missing values...")
        # Fill numeric columns with median
        numeric_cols = X.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            X[col] = X[col].fillna(X[col].median())
    
    # Ensure all data is numeric
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')
    
    # Ensure target is numeric and remove any NaN
    y = pd.to_numeric(y, errors='coerce')
    valid_indices = ~y.isna()
    X = X[valid_indices]
    y = y[valid_indices]
    
    print(f"After cleaning - Features: {X.shape}, Target: {y.shape}")
    print(f"ğŸ¯ Final target distribution: {y.value_counts().to_dict()}")
    
    # Check if we have enough samples for stratification
    min_class_size = y.value_counts().min()
    if min_class_size < 2:
        print("âš ï¸ Very small class detected, using simple split")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
    
    print(f"âœ… Data preprocessed successfully!")
    print(f"ğŸ“Š Train set: {X_train.shape}, Test set: {X_test.shape}")
    return X_train, X_test, y_train, y_test, feature_cols

def train_model(X_train, X_test, y_train, y_test, model_type, enable_tuning):
    """Train the selected model"""
    print(f"ğŸ¯ Training model: {model_type}")
    
    models = {
        "logistic": LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),
        "random_forest": RandomForestClassifier(random_state=42, n_estimators=100, class_weight='balanced'),
        "neural_net": MLPClassifier(random_state=42, max_iter=1000, hidden_layer_sizes=(100, 50))
    }
    
    if model_type not in models:
        print(f"âš ï¸ Model type {model_type} not found, using Random Forest")
        model_type = "random_forest"
    
    model = models[model_type]
    
    print(f"Training {model_type}...")
    model.fit(X_train, y_train)
    print(f"âœ… {model_type} training completed!")
    
    # Make predictions
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else y_pred
    
    # Calculate metrics
    metrics = {
        "accuracy": accuracy_score(y_test, y_pred),
        "precision": precision_score(y_test, y_pred, average='weighted', zero_division=0),
        "recall": recall_score(y_test, y_pred, average='weighted', zero_division=0),
        "f1_score": f1_score(y_test, y_pred, average='weighted', zero_division=0),
    }
    
    # Calculate ROC AUC for binary classification
    unique_classes = np.unique(y_test)
    if len(unique_classes) == 2 and len(np.unique(y_pred_proba)) > 1:
        metrics["roc_auc"] = roc_auc_score(y_test, y_pred_proba)
    else:
        metrics["roc_auc"] = 0.5
    
    # Class-wise metrics for imbalanced dataset
    if len(unique_classes) == 2:
        metrics["precision_class_0"] = precision_score(y_test, y_pred, pos_label=0, zero_division=0)
        metrics["recall_class_0"] = recall_score(y_test, y_pred, pos_label=0, zero_division=0)
        metrics["precision_class_1"] = precision_score(y_test, y_pred, pos_label=1, zero_division=0)
        metrics["recall_class_1"] = recall_score(y_test, y_pred, pos_label=1, zero_division=0)
    
    print("ğŸ“ˆ Model Metrics:")
    for key, value in metrics.items():
        print(f"  {key}: {value:.4f}")
    
    # Log to MLflow
    mlflow.sklearn.log_model(model, "model")
    mlflow.log_param("model_type", model_type)
    mlflow.log_param("enable_tuning", enable_tuning)
    mlflow.log_param("dataset", "diabetes_prediction")
    mlflow.log_param("target", "Diabetes_binary")
    
    for key, value in metrics.items():
        mlflow.log_metric(key, value)
    
    print("âœ… Model training and logging completed")
    return model, metrics

def save_results(results, output_path):
    """Save results to DBFS"""
    print("ğŸ’¾ Saving results...")
    
    try:
        results_json = json.dumps(results, indent=2)
        dbutils.fs.mkdirs(output_path)
        dbfs_output_path = f"{output_path}/results.json"
        dbutils.fs.put(dbfs_output_path, results_json, overwrite=True)
        print(f"âœ… Results saved to: {dbfs_output_path}")
        
        if 'metrics' in results and 'accuracy' in results['metrics']:
            accuracy = results['metrics']['accuracy']
            print(f"ğŸ“Š Final Accuracy: {accuracy:.4f}")
            if 'roc_auc' in results['metrics']:
                print(f"ğŸ“ˆ ROC AUC: {results['metrics']['roc_auc']:.4f}")
        
    except Exception as e:
        print(f"âŒ Error saving results: {e}")
        raise e

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¯ Main Pipeline

# COMMAND ----------

def main_pipeline():
    """Main pipeline execution"""
    try:
        # Get parameters from widgets
        table_name = dbutils.widgets.get("table_name")
        model_type = dbutils.widgets.get("model_type")
        enable_tuning = dbutils.widgets.get("enable_tuning")
        test_size = float(dbutils.widgets.get("test_size")) / 100
        output_path = dbutils.widgets.get("output_path")
        
        print("=" * 60)
        print("ğŸš€ Starting Diabetes Prediction Pipeline")
        print("=" * 60)
        print(f"ğŸ“Š Table Name: {table_name}")
        print(f"ğŸ¯ Model: {model_type}")
        print(f"ğŸ“ˆ Test Size: {test_size}")
        print(f"âš™ï¸ Tuning: {enable_tuning}")
        
        # Start MLflow experiment
        experiment_name = f"/Diabetes_Prediction_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        mlflow.set_experiment(experiment_name)
        
        with mlflow.start_run():
            # Log parameters
            mlflow.log_param("model_type", model_type)
            mlflow.log_param("enable_tuning", enable_tuning)
            mlflow.log_param("test_size", test_size)
            mlflow.log_param("table_name", table_name)
            mlflow.log_param("problem_type", "binary_classification")
            
            # Step 1: Load data from Delta Table
            print("\nğŸ“ Step 1: Loading data from Delta Table...")
            df = load_data_from_table(table_name)
            
            # Step 2: Perform EDA
            print("\nğŸ“Š Step 2: Exploratory Data Analysis")
            eda_results = perform_eda(df)
            mlflow.log_dict(eda_results, "eda_results.json")
            
            # Step 3: Preprocess data
            print("\nğŸ”§ Step 3: Data Preprocessing")
            X_train, X_test, y_train, y_test, feature_cols = preprocess_data(df, test_size)
            
            # Step 4: Train model
            print("\nğŸ¤– Step 4: Model Training")
            model, metrics = train_model(X_train, X_test, y_train, y_test, model_type, enable_tuning)
            
            # Step 5: Save results
            print("\nğŸ’¾ Step 5: Saving Results")
            results = {
                "model_type": model_type,
                "metrics": metrics,
                "eda": eda_results,
                "data_source": table_name,
                "timestamp": datetime.now().isoformat(),
                "status": "success",
                "problem_type": "diabetes_prediction"
            }
            
            save_results(results, output_path)
            mlflow.log_dict(results, "final_results.json")
            
            print("=" * 60)
            print("âœ… Diabetes Prediction Pipeline completed successfully!")
            print("=" * 60)
            
            return results
            
    except Exception as e:
        print("âŒ Pipeline failed!")
        print(f"Error: {str(e)}")
        
        error_results = {
            "status": "failed",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }
        
        try:
            save_results(error_results, output_path)
        except Exception as save_error:
            print(f"Could not save error results: {save_error}")
        
        raise e

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸƒâ€â™‚ï¸ Execute Pipeline

# COMMAND ----------

# Execute the pipeline
final_results = main_pipeline()

if final_results and final_results.get("status") == "success":
    print("\nğŸ‰ DIABETES PREDICTION PIPELINE COMPLETED!")
    print(f"ğŸ“Š Model: {final_results['model_type']}")
    print(f"ğŸ“ˆ Accuracy: {final_results['metrics']['accuracy']:.4f}")
    
    print("\nğŸ“Š FINAL METRICS:")
    for metric, value in final_results['metrics'].items():
        if not metric.startswith('precision_class_') and not metric.startswith('recall_class_'):
            print(f"  {metric}: {value:.4f}")
    
    # Show class-wise metrics if available
    if 'precision_class_0' in final_results['metrics']:
        print("\nğŸ¯ CLASS-WISE METRICS (Diabetes Prediction):")
        print(f"  No Diabetes (0) - Precision: {final_results['metrics']['precision_class_0']:.4f}, Recall: {final_results['metrics']['recall_class_0']:.4f}")
        print(f"  Has Diabetes (1) - Precision: {final_results['metrics']['precision_class_1']:.4f}, Recall: {final_results['metrics']['recall_class_1']:.4f}")
    
else:
    print("\nğŸ’¥ PIPELINE EXECUTION FAILED!")
